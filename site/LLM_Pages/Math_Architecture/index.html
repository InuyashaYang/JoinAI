
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../../agent/">
      
      
        <link rel="next" href="../Position_Embedding/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.36">
    
    
      
        <title>Transformer数学架构 - AIDIY Wiki</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.06209087.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#0-transformer" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="AIDIY Wiki" class="md-header__button md-logo" aria-label="AIDIY Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            AIDIY Wiki
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Transformer数学架构
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../agent/" class="md-tabs__link">
        
  
    
  
  Agent

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="./" class="md-tabs__link">
          
  
  LLM

        </a>
      </li>
    
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../stablediffusion/" class="md-tabs__link">
        
  
    
  
  StableDiffusion

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../vit/" class="md-tabs__link">
        
  
    
  
  ViT

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="AIDIY Wiki" class="md-nav__button md-logo" aria-label="AIDIY Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    AIDIY Wiki
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../agent/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Agent
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  <span class="md-ellipsis">
    LLM
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            LLM
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Transformer数学架构
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Transformer数学架构
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#0-transformer" class="md-nav__link">
    <span class="md-ellipsis">
      0 大模型 Transformer 算法架构
    </span>
  </a>
  
    <nav class="md-nav" aria-label="0 大模型 Transformer 算法架构">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformer-encoder" class="md-nav__link">
    <span class="md-ellipsis">
      Transformer 编码器（Encoder）结构
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformer-decoder" class="md-nav__link">
    <span class="md-ellipsis">
      Transformer 解码器（Decoder）结构
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transformer" class="md-nav__link">
    <span class="md-ellipsis">
      Transformer中的数据流动
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      模型更新时更新的参数
    </span>
  </a>
  
    <nav class="md-nav" aria-label="模型更新时更新的参数">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#encoder" class="md-nav__link">
    <span class="md-ellipsis">
      Encoder
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#decoder" class="md-nav__link">
    <span class="md-ellipsis">
      Decoder
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      一些问题
    </span>
  </a>
  
    <nav class="md-nav" aria-label="一些问题">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      为什么在编码器和解码器层中有多次残差连接+层归一化？
    </span>
  </a>
  
    <nav class="md-nav" aria-label="为什么在编码器和解码器层中有多次残差连接+层归一化？">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      编码器层
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      解码器层
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    <span class="md-ellipsis">
      总结
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ffn-transformer" class="md-nav__link">
    <span class="md-ellipsis">
      前馈神经网络（FFN）在 Transformer 中的作用
    </span>
  </a>
  
    <nav class="md-nav" aria-label="前馈神经网络（FFN）在 Transformer 中的作用">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ffn" class="md-nav__link">
    <span class="md-ellipsis">
      编码器中的 FFN
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ffn_1" class="md-nav__link">
    <span class="md-ellipsis">
      解码器中的 FFN
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    <span class="md-ellipsis">
      共同特点
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    <span class="md-ellipsis">
      总结
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Position_Embedding/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    位置编码
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Pre-Train/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    预训练阶段
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../SFT/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    SFT阶段
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../RLHF/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    RLHF阶段
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Post-Train/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    后训练阶段
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../stablediffusion/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    StableDiffusion
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../vit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ViT
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#0-transformer" class="md-nav__link">
    <span class="md-ellipsis">
      0 大模型 Transformer 算法架构
    </span>
  </a>
  
    <nav class="md-nav" aria-label="0 大模型 Transformer 算法架构">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformer-encoder" class="md-nav__link">
    <span class="md-ellipsis">
      Transformer 编码器（Encoder）结构
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformer-decoder" class="md-nav__link">
    <span class="md-ellipsis">
      Transformer 解码器（Decoder）结构
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transformer" class="md-nav__link">
    <span class="md-ellipsis">
      Transformer中的数据流动
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      模型更新时更新的参数
    </span>
  </a>
  
    <nav class="md-nav" aria-label="模型更新时更新的参数">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#encoder" class="md-nav__link">
    <span class="md-ellipsis">
      Encoder
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#decoder" class="md-nav__link">
    <span class="md-ellipsis">
      Decoder
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      一些问题
    </span>
  </a>
  
    <nav class="md-nav" aria-label="一些问题">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      为什么在编码器和解码器层中有多次残差连接+层归一化？
    </span>
  </a>
  
    <nav class="md-nav" aria-label="为什么在编码器和解码器层中有多次残差连接+层归一化？">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      编码器层
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      解码器层
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    <span class="md-ellipsis">
      总结
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ffn-transformer" class="md-nav__link">
    <span class="md-ellipsis">
      前馈神经网络（FFN）在 Transformer 中的作用
    </span>
  </a>
  
    <nav class="md-nav" aria-label="前馈神经网络（FFN）在 Transformer 中的作用">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ffn" class="md-nav__link">
    <span class="md-ellipsis">
      编码器中的 FFN
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ffn_1" class="md-nav__link">
    <span class="md-ellipsis">
      解码器中的 FFN
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    <span class="md-ellipsis">
      共同特点
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    <span class="md-ellipsis">
      总结
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<p><a href="https://github.com/InuyashaYang/JoinAI"><img alt="GitHub stars" src="https://img.shields.io/github/stars/InuyashaYang/JoinAI?style=social" /></a></p>
<h2 id="0-transformer">0 大模型 Transformer 算法架构<a class="headerlink" href="#0-transformer" title="Permanent link">&para;</a></h2>
<h3 id="transformer-encoder">Transformer 编码器（Encoder）结构<a class="headerlink" href="#transformer-encoder" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>层</th>
<th>输入</th>
<th>操作</th>
<th>输出</th>
</tr>
</thead>
<tbody>
<tr>
<td>输入嵌入</td>
<td>原始输入序列</td>
<td>词嵌入</td>
<td><span class="arithmatex">\(X \in \mathbb{R}^{L \times d_{model}}\)</span></td>
</tr>
<tr>
<td>位置编码</td>
<td>词嵌入 <span class="arithmatex">\(X \in \mathbb{R}^{L \times d_{model}}\)</span></td>
<td>添加位置信息：<span class="arithmatex">\(I = X + P\)</span> <br> 其中 <span class="arithmatex">\(P \in \mathbb{R}^{L \times d_{model}}\)</span> 是位置编码矩阵</td>
<td><span class="arithmatex">\(I \in \mathbb{R}^{L \times d_{model}}\)</span></td>
</tr>
<tr>
<td>多头自注意力</td>
<td><span class="arithmatex">\(I \in \mathbb{R}^{L \times d_{model}}\)</span></td>
<td>1. <span class="arithmatex">\(Q = IW_Q, K = IW_K, V = IW_V\)</span> <br> 2. <span class="arithmatex">\(A = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})\)</span> <br> 3. <span class="arithmatex">\(Z = AV\)</span> <br> 4. 拼接多头: <span class="arithmatex">\(Z_{concat} = [Z_1; Z_2; ...; Z_h]\)</span> <br> 5. <span class="arithmatex">\(\text{MultiHead} = Z_{concat}W_O\)</span> <br> 其中所有中间结果维度与输入输出一致</td>
<td><span class="arithmatex">\(X_{attn} = \text{MultiHead}(Q,K,V) \in \mathbb{R}^{L \times d_{model}}\)</span></td>
</tr>
<tr>
<td>残差连接 + 层归一化</td>
<td>多头注意力输出 <span class="arithmatex">\(X_{attn} \in \mathbb{R}^{L \times d_{model}}\)</span> <br> 原始输入 <span class="arithmatex">\(I \in \mathbb{R}^{L \times d_{model}}\)</span></td>
<td>1. 残差连接：<span class="arithmatex">\(X_{residual} = X_{attn} + I\)</span> <br><br> 2. 计算均值和标准差：<br> <span class="arithmatex">\(\mu = \frac{1}{d_{model}}\sum_{j=1}^{d_{model}} X_{residual_{:,j}} \in \mathbb{R}^{L \times 1}\)</span> <br> <span class="arithmatex">\(\sigma = \sqrt{\frac{1}{d_{model}}\sum_{j=1}^{d_{model}} (X_{residual_{:,j}} - \mu)^2} \in \mathbb{R}^{L \times 1}\)</span> <br><br> 3. 层归一化：<br> <span class="arithmatex">\(X_{norm} = \gamma \odot (\frac{X_{residual} - \mu \mathbf{1}^T}{(\sigma + \varepsilon)\mathbf{1}^T}) + \beta\)</span> <br><br> 其中：<br> - <span class="arithmatex">\(\mathbf{1} \in \mathbb{R}^{1 \times d_{model}}\)</span> 是全1向量<br> - <span class="arithmatex">\(\gamma, \beta \in \mathbb{R}^{1 \times d_{model}}\)</span> 是可学习参数<br> - <span class="arithmatex">\(\odot\)</span> 表示逐元素乘法（使用广播）<br> - 除法也是逐元素操作（使用广播）</td>
<td><span class="arithmatex">\(X_{norm} \in \mathbb{R}^{L \times d_{model}}\)</span></td>
</tr>
<tr>
<td>前馈神经网络 (FFN)</td>
<td>上一层输出 <span class="arithmatex">\(X_{norm} \in \mathbb{R}^{L \times d_{model}}\)</span></td>
<td><span class="arithmatex">\(FFN(X_{norm}) = \max(0, X_{norm}W_1 + b_1)W_2 + b_2\)</span> <br> 其中 <span class="arithmatex">\(W_1 \in \mathbb{R}^{d_{model} \times d_{ff}}\)</span>, <span class="arithmatex">\(W_2 \in \mathbb{R}^{d_{ff} \times d_{model}}\)</span> <br> <span class="arithmatex">\(b_1 \in \mathbb{R}^{1 \times d_{ff}}\)</span>, <span class="arithmatex">\(b_2 \in \mathbb{R}^{1 \times d_{model}}\)</span> <br> <span class="arithmatex">\(d_{ff}\)</span> 通常大于 <span class="arithmatex">\(d_{model}\)</span>，例如 <span class="arithmatex">\(d_{ff} = 4 \times d_{model}\)</span></td>
<td><span class="arithmatex">\(X_{ffn} \in \mathbb{R}^{L \times d_{model}}\)</span></td>
</tr>
<tr>
<td>残差连接 + 层归一化</td>
<td>FFN输出 <span class="arithmatex">\(X_{ffn} \in \mathbb{R}^{L \times d_{model}}\)</span> <br> FFN输入 <span class="arithmatex">\(X_{norm} \in \mathbb{R}^{L \times d_{model}}\)</span></td>
<td>1. 残差连接：<span class="arithmatex">\(X_{residual} = X_{ffn} + X_{norm}\)</span> <br><br> 2. 计算均值和标准差：<br> <span class="arithmatex">\(\mu = \frac{1}{d_{model}}\sum_{j=1}^{d_{model}} X_{residual_{:,j}} \in \mathbb{R}^{L \times 1}\)</span> <br> <span class="arithmatex">\(\sigma = \sqrt{\frac{1}{d_{model}}\sum_{j=1}^{d_{model}} (X_{residual_{:,j}} - \mu)^2} \in \mathbb{R}^{L \times 1}\)</span> <br><br> 3. 层归一化：<br> <span class="arithmatex">\(X_{out} = \gamma \odot (\frac{X_{residual} - \mu \mathbf{1}^T}{(\sigma + \varepsilon)\mathbf{1}^T}) + \beta\)</span> <br><br></td>
<td><span class="arithmatex">\(X_{out} \in \mathbb{R}^{L \times d_{model}}\)</span></td>
</tr>
</tbody>
</table>
<p>注意：</p>
<ol>
<li>
<p><span class="arithmatex">\(L\)</span> 是输入序列的长度</p>
</li>
<li>
<p><span class="arithmatex">\(d_{model}\)</span> 是模型的隐藏维度</p>
</li>
<li>
<p>多头注意力中，<span class="arithmatex">\(d_k = d_{model} / \text{num\_heads}\)</span></p>
</li>
<li>
<p>层归一化 (LN) 公式：<span class="arithmatex">\(\text{LN}(x) = \gamma \odot \frac{x - \mu}{\sigma + \epsilon} + \beta\)</span></p>
</li>
</ol>
<details> <summary>⊙ Hadamard乘积（元素级乘法）解释</summary>
Hadamard乘积，也称为元素级乘法（element-wise multiplication），是两个相同维度的矩阵或向量之间的运算。

定义：
对于两个相同维度的矩阵 A 和 B，它们的Hadamard乘积 C = A ⊙ B 定义为：
C[i,j] = A[i,j] * B[i,j]

其中 [i,j] 表示矩阵的第i行第j列元素。

例子：
假设有两个 2x2 矩阵：

A = [1 2]
[3 4]

B = [5 6]
[7 8]

它们的Hadamard乘积为：

C = A ⊙ B = [15 26]
[37 48]
= [5 12]
[21 32]

特点：

结果矩阵与原矩阵维度相同
每个位置的元素是原矩阵对应位置元素的乘积
交换律成立：A ⊙ B = B ⊙ A
与普通矩阵乘法不同
在深度学习中的应用：

在注意力机制中进行缩放
在某些激活函数的计算中
在梯度计算和反向传播中
编程实现：
在numpy中可以直接用 * 操作符：C = A * B
在PyTorch中可以用 torch.mul(A, B) 或 A * B

</details>

<h3 id="transformer-decoder">Transformer 解码器（Decoder）结构<a class="headerlink" href="#transformer-decoder" title="Permanent link">&para;</a></h3>
<p>注意：整个解码器结构以编码器输出 <span class="arithmatex">\(E \in \mathbb{R}^{L \times d_{model}}\)</span> 作为额外输入，用于交叉注意力层。</p>
<table>
<thead>
<tr>
<th>层</th>
<th>输入</th>
<th>操作</th>
<th>输出</th>
</tr>
</thead>
<tbody>
<tr>
<td>输入嵌入</td>
<td>目标序列</td>
<td>词嵌入</td>
<td><span class="arithmatex">\(Y \in \mathbb{R}^{T \times d_{model}}\)</span></td>
</tr>
<tr>
<td>位置编码</td>
<td>词嵌入 <span class="arithmatex">\(Y\)</span></td>
<td>添加位置信息</td>
<td><span class="arithmatex">\(I = Y + P \in \mathbb{R}^{T \times d_{model}}\)</span></td>
</tr>
<tr>
<td>掩码多头自注意力</td>
<td><span class="arithmatex">\(I\)</span></td>
<td>1. <span class="arithmatex">\(Q = IW_Q, K = IW_K, V = IW_V\)</span> <br> 2. <span class="arithmatex">\(A = \text{softmax}(\frac{QK^T}{\sqrt{d_k}} + M)\)</span> <br> 3. <span class="arithmatex">\(Z = AV\)</span> <br> 4. 拼接多头: <span class="arithmatex">\(Z_{concat} = [Z_1; Z_2; ...; Z_h]\)</span> <br> 5. <span class="arithmatex">\(\text{MaskedMultiHead} = Z_{concat}W_O\)</span> <br><br> 其中 <span class="arithmatex">\(M\)</span> 是掩码矩阵，用于防止关注未来位置</td>
<td><span class="arithmatex">\(X_{attn} = \text{MaskedMultiHead}(Q,K,V) \in \mathbb{R}^{T \times d_{model}}\)</span></td>
</tr>
<tr>
<td>残差连接 + 层归一化</td>
<td>掩码多头注意力输出 <span class="arithmatex">\(X_{attn}\)</span> <br> 原始输入 <span class="arithmatex">\(I\)</span></td>
<td>1. 残差连接：<span class="arithmatex">\(X_{residual} = X_{attn} + I\)</span> <br> 2. 计算均值和标准差：<br> <span class="arithmatex">\(\mu = \frac{1}{d_{model}}\sum_{j=1}^{d_{model}} X_{residual_{:,j}} \in \mathbb{R}^{T \times 1}\)</span> <br> <span class="arithmatex">\(\sigma = \sqrt{\frac{1}{d_{model}}\sum_{j=1}^{d_{model}} (X_{residual_{:,j}} - \mu)^2} \in \mathbb{R}^{T \times 1}\)</span> <br> 3. 层归一化：<br> <span class="arithmatex">\(X_{norm} = \gamma \odot (\frac{X_{residual} - \mu \mathbf{1}^T}{(\sigma + \varepsilon)\mathbf{1}^T}) + \beta\)</span> <br><br> 其中：<br> - <span class="arithmatex">\(\mathbf{1} \in \mathbb{R}^{1 \times d_{model}}\)</span> 是全1向量<br> - <span class="arithmatex">\(\gamma, \beta \in \mathbb{R}^{1 \times d_{model}}\)</span> 是可学习参数<br> - <span class="arithmatex">\(\odot\)</span> 表示逐元素乘法（使用广播）<br> - 除法也是逐元素操作（使用广播）</td>
<td><span class="arithmatex">\(X_{norm} \in \mathbb{R}^{T \times d_{model}}\)</span></td>
</tr>
<tr>
<td>多头交叉注意力</td>
<td><span class="arithmatex">\(X_{norm}\)</span>, 编码器输出 <span class="arithmatex">\(E\)</span></td>
<td>1. <span class="arithmatex">\(Q = X_{norm}W_Q\)</span> <br> 2. <span class="arithmatex">\(K = EW_K\)</span> <br> 3. <span class="arithmatex">\(V = EW_V\)</span> <br> 4. <span class="arithmatex">\(A = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})\)</span> <br> 5. <span class="arithmatex">\(Z = AV\)</span> <br> 6. 拼接多头: <span class="arithmatex">\(Z_{concat} = [Z_1; Z_2; ...; Z_h]\)</span> <br> 7. <span class="arithmatex">\(\text{CrossMultiHead} = Z_{concat}W_O\)</span></td>
<td><span class="arithmatex">\(X_{cross} = \text{CrossMultiHead}(Q,K,V) \in \mathbb{R}^{T \times d_{model}}\)</span></td>
</tr>
<tr>
<td>残差连接 + 层归一化</td>
<td>交叉注意力输出 <span class="arithmatex">\(X_{cross}\)</span> <br> 上一层输出 <span class="arithmatex">\(X_{norm}\)</span></td>
<td>1. 残差连接：<span class="arithmatex">\(X_{residual} = X_{cross} + X_{norm}\)</span> <br> 2. 计算均值和标准差：<br> <span class="arithmatex">\(\mu = \frac{1}{d_{model}}\sum_{j=1}^{d_{model}} X_{residual_{:,j}} \in \mathbb{R}^{T \times 1}\)</span> <br> <span class="arithmatex">\(\sigma = \sqrt{\frac{1}{d_{model}}\sum_{j=1}^{d_{model}} (X_{residual_{:,j}} - \mu)^2} \in \mathbb{R}^{T \times 1}\)</span> <br> 3. 层归一化：<br> <span class="arithmatex">\(X_{norm2} = \gamma \odot (\frac{X_{residual} - \mu \mathbf{1}^T}{(\sigma + \varepsilon)\mathbf{1}^T}) + \beta\)</span></td>
<td><span class="arithmatex">\(X_{norm2} \in \mathbb{R}^{T \times d_{model}}\)</span></td>
</tr>
<tr>
<td>前馈神经网络 (FFN)</td>
<td><span class="arithmatex">\(X_{norm2}\)</span></td>
<td><span class="arithmatex">\(FFN(X_{norm2}) = \max(0, X_{norm2}W_1 + b_1)W_2 + b_2\)</span> <br> 其中 <span class="arithmatex">\(W_1 \in \mathbb{R}^{d_{model} \times d_{ff}}\)</span>, <span class="arithmatex">\(W_2 \in \mathbb{R}^{d_{ff} \times d_{model}}\)</span> <br> <span class="arithmatex">\(b_1 \in \mathbb{R}^{1 \times d_{ff}}\)</span>, <span class="arithmatex">\(b_2 \in \mathbb{R}^{1 \times d_{model}}\)</span> <br> <span class="arithmatex">\(d_{ff}\)</span> 通常大于 <span class="arithmatex">\(d_{model}\)</span>，例如 <span class="arithmatex">\(d_{ff} = 4 \times d_{model}\)</span></td>
<td><span class="arithmatex">\(X_{ffn} \in \mathbb{R}^{T \times d_{model}}\)</span></td>
</tr>
<tr>
<td>残差连接 + 层归一化</td>
<td>FFN输出 <span class="arithmatex">\(X_{ffn}\)</span> <br> FFN输入 <span class="arithmatex">\(X_{norm2}\)</span></td>
<td>1. 残差连接：<span class="arithmatex">\(X_{residual} = X_{ffn} + X_{norm2}\)</span> <br> 2. 计算均值和标准差：<br> <span class="arithmatex">\(\mu = \frac{1}{d_{model}}\sum_{j=1}^{d_{model}} X_{residual_{:,j}} \in \mathbb{R}^{T \times 1}\)</span> <br> <span class="arithmatex">\(\sigma = \sqrt{\frac{1}{d_{model}}\sum_{j=1}^{d_{model}} (X_{residual_{:,j}} - \mu)^2} \in \mathbb{R}^{T \times 1}\)</span> <br> 3. 层归一化：<br> <span class="arithmatex">\(X_{out} = \gamma \odot (\frac{X_{residual} - \mu \mathbf{1}^T}{(\sigma + \varepsilon)\mathbf{1}^T}) + \beta\)</span></td>
<td><span class="arithmatex">\(X_{out} \in \mathbb{R}^{T \times d_{model}}\)</span></td>
</tr>
<tr>
<td>线性层(最后一层decoder)</td>
<td><span class="arithmatex">\(X_{out}\)</span></td>
<td><span class="arithmatex">\(\text{Linear}(X_{out}) = X_{out}W + b\)</span> <br> 其中 <span class="arithmatex">\(W \in \mathbb{R}^{d_{model} \times \text{vocab\_size}}\)</span></td>
<td><span class="arithmatex">\(\in \mathbb{R}^{T \times \text{vocab\_size}}\)</span></td>
</tr>
<tr>
<td>Softmax(最后一层decoder)</td>
<td>线性层输出</td>
<td><span class="arithmatex">\(\text{softmax}(x)_i = \frac{\exp(x_i)}{\sum_j \exp(x_j)}\)</span> <br> 对每个时间步的输出进行softmax操作</td>
<td><span class="arithmatex">\(\in \mathbb{R}^{T \times \text{vocab\_size}}\)</span></td>
</tr>
</tbody>
</table>
<p>注意：</p>
<ol>
<li>
<p><span class="arithmatex">\(T\)</span> 是目标序列的长度</p>
</li>
<li>
<p><span class="arithmatex">\(L\)</span> 是源序列的长度</p>
</li>
<li>
<p><span class="arithmatex">\(d_{model}\)</span> 是模型的隐藏维度</p>
</li>
<li>
<p>多头注意力中，<span class="arithmatex">\(d_k = d_{model} / \text{num\_heads}\)</span></p>
</li>
<li>
<p>层归一化 (LN) 公式：<span class="arithmatex">\(\text{LN}(x) = \gamma \odot \frac{x - \mu}{\sigma + \epsilon} + \beta\)</span></p>
</li>
<li>
<p>解码器中的掩码多头自注意力使用了掩码矩阵 <span class="arithmatex">\(M\)</span>，以防止关注未来位置</p>
</li>
<li>
<p>交叉注意力层使用编码器的输出 <span class="arithmatex">\(E\)</span> 作为 Key 和 Value</p>
</li>
</ol>
<h1 id="transformer">Transformer中的数据流动<a class="headerlink" href="#transformer" title="Permanent link">&para;</a></h1>
<table>
<thead>
<tr>
<th>层</th>
<th>输入</th>
<th>输出</th>
<th>输出去向</th>
</tr>
</thead>
<tbody>
<tr>
<td>输入嵌入</td>
<td>源序列</td>
<td><span class="arithmatex">\(X_0 \in \mathbb{R}^{L \times d_{model}}\)</span></td>
<td>Encoder第1层</td>
</tr>
<tr>
<td>Encoder第1层</td>
<td><span class="arithmatex">\(X_0\)</span></td>
<td><span class="arithmatex">\(X_1 \in \mathbb{R}^{L \times d_{model}}\)</span></td>
<td>Encoder第2层</td>
</tr>
<tr>
<td>Encoder第2-5层</td>
<td>上一层输出</td>
<td><span class="arithmatex">\(X_i \in \mathbb{R}^{L \times d_{model}}\)</span></td>
<td>下一个Encoder层</td>
</tr>
<tr>
<td>Encoder第6层</td>
<td><span class="arithmatex">\(X_5\)</span></td>
<td><span class="arithmatex">\(E \in \mathbb{R}^{L \times d_{model}}\)</span></td>
<td>所有Decoder层的交叉注意力</td>
</tr>
<tr>
<td>目标嵌入</td>
<td>目标序列</td>
<td><span class="arithmatex">\(Y_0 \in \mathbb{R}^{T \times d_{model}}\)</span></td>
<td>Decoder第1层</td>
</tr>
<tr>
<td>Decoder第1层</td>
<td><span class="arithmatex">\(Y_0\)</span>, <span class="arithmatex">\(E\)</span></td>
<td><span class="arithmatex">\(Y_1 \in \mathbb{R}^{T \times d_{model}}\)</span></td>
<td>Decoder第2层</td>
</tr>
<tr>
<td>Decoder第2-5层</td>
<td>上一层输出, <span class="arithmatex">\(E\)</span></td>
<td><span class="arithmatex">\(Y_i \in \mathbb{R}^{T \times d_{model}}\)</span></td>
<td>下一个Decoder层</td>
</tr>
<tr>
<td>Decoder第6层</td>
<td><span class="arithmatex">\(Y_5\)</span>, <span class="arithmatex">\(E\)</span></td>
<td><span class="arithmatex">\(Y_6 \in \mathbb{R}^{T \times d_{model}}\)</span></td>
<td>线性层</td>
</tr>
<tr>
<td>线性层</td>
<td><span class="arithmatex">\(Y_6\)</span></td>
<td><span class="arithmatex">\(Z \in \mathbb{R}^{T \times \text{vocab\_size}}\)</span></td>
<td>Softmax</td>
</tr>
<tr>
<td>Softmax</td>
<td><span class="arithmatex">\(Z\)</span></td>
<td>概率分布 <span class="arithmatex">\(\in \mathbb{R}^{T \times \text{vocab\_size}}\)</span></td>
<td>最终输出</td>
</tr>
</tbody>
</table>
<p>注：
- <span class="arithmatex">\(L\)</span>: 源序列长度
- <span class="arithmatex">\(T\)</span>: 目标序列长度
- <span class="arithmatex">\(d_{model}\)</span>: 模型维度
- <span class="arithmatex">\(\text{vocab\_size}\)</span>: 词汇表大小</p>
<h1 id="_1">模型更新时更新的参数<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h1>
<h2 id="encoder">Encoder<a class="headerlink" href="#encoder" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th>组件</th>
<th>参数</th>
<th>维度</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>多头自注意力</td>
<td><span class="arithmatex">\(W_Q\)</span></td>
<td><span class="arithmatex">\(\mathbb{R}^{d_{model} \times d_k}\)</span></td>
<td>查询权重（每个头）</td>
</tr>
<tr>
<td></td>
<td><span class="arithmatex">\(W_K\)</span></td>
<td><span class="arithmatex">\(\mathbb{R}^{d_{model} \times d_k}\)</span></td>
<td>键权重（每个头）</td>
</tr>
<tr>
<td></td>
<td><span class="arithmatex">\(W_V\)</span></td>
<td><span class="arithmatex">\(\mathbb{R}^{d_{model} \times d_v}\)</span></td>
<td>值权重（每个头）</td>
</tr>
<tr>
<td></td>
<td><span class="arithmatex">\(W_O\)</span></td>
<td><span class="arithmatex">\(\mathbb{R}^{hd_v \times d_{model}}\)</span></td>
<td>输出权重</td>
</tr>
<tr>
<td>层归一化 (注意力后)</td>
<td><span class="arithmatex">\(\gamma\)</span></td>
<td><span class="arithmatex">\(\mathbb{R}^{d_{model}}\)</span></td>
<td>缩放参数</td>
</tr>
<tr>
<td></td>
<td><span class="arithmatex">\(\beta\)</span></td>
<td><span class="arithmatex">\(\mathbb{R}^{d_{model}}\)</span></td>
<td>偏移参数</td>
</tr>
<tr>
<td>前馈神经网络</td>
<td><span class="arithmatex">\(W_1\)</span></td>
<td><span class="arithmatex">\(\mathbb{R}^{d_{model} \times d_{ff}}\)</span></td>
<td>第一层权重</td>
</tr>
<tr>
<td></td>
<td><span class="arithmatex">\(b_1\)</span></td>
<td><span class="arithmatex">\(\mathbb{R}^{d_{ff}}\)</span></td>
<td>第一层偏置</td>
</tr>
<tr>
<td></td>
<td><span class="arithmatex">\(W_2\)</span></td>
<td><span class="arithmatex">\(\mathbb{R}^{d_{ff} \times d_{model}}\)</span></td>
<td>第二层权重</td>
</tr>
<tr>
<td></td>
<td><span class="arithmatex">\(b_2\)</span></td>
<td><span class="arithmatex">\(\mathbb{R}^{d_{model}}\)</span></td>
<td>第二层偏置</td>
</tr>
<tr>
<td>层归一化 (FFN后)</td>
<td><span class="arithmatex">\(\gamma\)</span></td>
<td><span class="arithmatex">\(\mathbb{R}^{d_{model}}\)</span></td>
<td>缩放参数</td>
</tr>
<tr>
<td></td>
<td><span class="arithmatex">\(\beta\)</span></td>
<td><span class="arithmatex">\(\mathbb{R}^{d_{model}}\)</span></td>
<td>偏移参数</td>
</tr>
</tbody>
</table>
<p>注意：
- <span class="arithmatex">\(L\)</span> 是输入序列的长度
- <span class="arithmatex">\(h\)</span> 是注意力头的数量
- <span class="arithmatex">\(d_k\)</span> 和 <span class="arithmatex">\(d_v\)</span> 通常等于 <span class="arithmatex">\(d_{model} / h\)</span>
- <span class="arithmatex">\(d_{ff}\)</span> 是前馈网络的内部维度，通常大于 <span class="arithmatex">\(d_{model}\)</span>
- 除了输入嵌入，其他参数在每个编码器层中都会重复出现
- 如果编码器有多层（例如标准 Transformer 中的 6 层），这些参数会在每层中重复</p>
<h2 id="decoder">Decoder<a class="headerlink" href="#decoder" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th>组件</th>
<th>参数</th>
<th>维度</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>掩码多头自注意力</td>
<td><span class="arithmatex">\(W_Q\)</span></td>
<td><span class="arithmatex">\(\mathbb{R}^{d_{model} \times d_k}\)</span></td>
<td>查询权重（每个头）</td>
</tr>
<tr>
<td></td>
<td><span class="arithmatex">\(W_K\)</span></td>
<td><span class="arithmatex">\(\mathbb{R}^{d_{model} \times d_k}\)</span></td>
<td>键权重（每个头）</td>
</tr>
<tr>
<td></td>
<td><span class="arithmatex">\(W_V\)</span></td>
<td><span class="arithmatex">\(\mathbb{R}^{d_{model} \times d_v}\)</span></td>
<td>值权重（每个头）</td>
</tr>
<tr>
<td></td>
<td><span class="arithmatex">\(W_O\)</span></td>
<td><span class="arithmatex">\(\mathbb{R}^{hd_v \times d_{model}}\)</span></td>
<td>输出权重</td>
</tr>
<tr>
<td>多头交叉注意力</td>
<td><span class="arithmatex">\(W_Q\)</span></td>
<td><span class="arithmatex">\(\mathbb{R}^{d_{model} \times d_k}\)</span></td>
<td>查询权重（每个头）</td>
</tr>
<tr>
<td></td>
<td><span class="arithmatex">\(W_K\)</span></td>
<td><span class="arithmatex">\(\mathbb{R}^{d_{model} \times d_k}\)</span></td>
<td>键权重（每个头）</td>
</tr>
<tr>
<td></td>
<td><span class="arithmatex">\(W_V\)</span></td>
<td><span class="arithmatex">\(\mathbb{R}^{d_{model} \times d_v}\)</span></td>
<td>值权重（每个头）</td>
</tr>
<tr>
<td></td>
<td><span class="arithmatex">\(W_O\)</span></td>
<td><span class="arithmatex">\(\mathbb{R}^{hd_v \times d_{model}}\)</span></td>
<td>输出权重</td>
</tr>
<tr>
<td>前馈神经网络</td>
<td><span class="arithmatex">\(W_1\)</span></td>
<td><span class="arithmatex">\(\mathbb{R}^{d_{model} \times d_{ff}}\)</span></td>
<td>第一层权重</td>
</tr>
<tr>
<td></td>
<td><span class="arithmatex">\(b_1\)</span></td>
<td><span class="arithmatex">\(\mathbb{R}^{d_{ff}}\)</span></td>
<td>第一层偏置</td>
</tr>
<tr>
<td></td>
<td><span class="arithmatex">\(W_2\)</span></td>
<td><span class="arithmatex">\(\mathbb{R}^{d_{ff} \times d_{model}}\)</span></td>
<td>第二层权重</td>
</tr>
<tr>
<td></td>
<td><span class="arithmatex">\(b_2\)</span></td>
<td><span class="arithmatex">\(\mathbb{R}^{d_{model}}\)</span></td>
<td>第二层偏置</td>
</tr>
<tr>
<td>层归一化</td>
<td><span class="arithmatex">\(\gamma\)</span></td>
<td><span class="arithmatex">\(\mathbb{R}^{d_{model}}\)</span></td>
<td>缩放参数</td>
</tr>
<tr>
<td></td>
<td><span class="arithmatex">\(\beta\)</span></td>
<td><span class="arithmatex">\(\mathbb{R}^{d_{model}}\)</span></td>
<td>偏移参数</td>
</tr>
<tr>
<td>输出线性层</td>
<td><span class="arithmatex">\(W_{out}\)</span></td>
<td><span class="arithmatex">\(\mathbb{R}^{d_{model} \times \text{vocab\_size}}\)</span></td>
<td>输出权重</td>
</tr>
<tr>
<td></td>
<td><span class="arithmatex">\(b_{out}\)</span></td>
<td><span class="arithmatex">\(\mathbb{R}^{\text{vocab\_size}}\)</span></td>
<td>输出偏置</td>
</tr>
</tbody>
</table>
<p>注意：</p>
<ul>
<li>
<p><span class="arithmatex">\(h\)</span> 是注意力头的数量</p>
</li>
<li>
<p><span class="arithmatex">\(d_k\)</span> 和 <span class="arithmatex">\(d_v\)</span> 通常等于 <span class="arithmatex">\(d_{model} / h\)</span></p>
</li>
<li>
<p><span class="arithmatex">\(d_{ff}\)</span> 是前馈网络的内部维度，通常大于 <span class="arithmatex">\(d_{model}\)</span></p>
</li>
<li>
<p>除了输入嵌入和最终输出层，其他参数在每个解码器层中都会重复出现</p>
</li>
<li>
<p>如果解码器有多层（例如标准 Transformer 中的 6 层），这些参数会在每层中重复</p>
</li>
</ul>
<h1 id="_2">一些问题<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h1>
<h2 id="_3">为什么在编码器和解码器层中有多次残差连接+层归一化？<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h2>
<h3 id="_4">编码器层<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>第一次残差连接+层归一化（在自注意力之后）</strong></li>
<li><strong>保持信息流动</strong>：允许原始信息直接传递，有助于解决深层网络中的梯度消失问题。</li>
<li><strong>稳定训练</strong>：层归一化有助于稳定深层网络的训练过程。</li>
<li>
<p><strong>增强特征</strong>：结合了自注意力机制的上下文信息和原始输入信息。</p>
</li>
<li>
<p><strong>第二次残差连接+层归一化（在前馈网络之后）</strong></p>
</li>
<li><strong>非线性转换</strong>：在保留原始信息的同时，允许模型进行复杂的非线性变换。</li>
<li><strong>深度特征提取</strong>：前馈网络可以提取更深层次的特征，残差连接确保这些特征与原始信息结合。</li>
<li><strong>梯度流动</strong>：再次缓解梯度消失问题，使得即使在很深的网络中，信息也能有效地向后传播。</li>
</ol>
<h3 id="_5">解码器层<a class="headerlink" href="#_5" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>第一次残差连接+层归一化（在掩码自注意力之后）</strong></li>
<li><strong>保持目标序列信息</strong>：确保模型在处理目标序列时不会丢失原始输入信息。</li>
<li>
<p><strong>自回归特性</strong>：在自回归生成过程中，有助于保持已生成部分的连贯性。</p>
</li>
<li>
<p><strong>第二次残差连接+层归一化（在交叉注意力之后）</strong></p>
</li>
<li><strong>融合编码器信息</strong>：允许解码器有效地整合来自编码器的信息，同时保留自身的表示。</li>
<li>
<p><strong>平衡源和目标信息</strong>：帮助模型在源语言（编码器输出）和目标语言（解码器状态）之间取得平衡。</p>
</li>
<li>
<p><strong>第三次残差连接+层归一化（在前馈网络之后）</strong></p>
</li>
<li><strong>功能类似编码器</strong>：进行深度特征提取和非线性变换。</li>
<li><strong>整合多源信息</strong>：结合了自注意力、交叉注意力和前馈网络的输出，形成丰富的表示。</li>
</ol>
<h2 id="_6">总结<a class="headerlink" href="#_6" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>信息流动</strong>：多次残差连接确保了原始信息和转换后的信息能够在深层网络中有效传播。</li>
<li><strong>梯度流动</strong>：有助于解决深度学习中的梯度消失问题，使得模型更容易训练。</li>
<li><strong>特征融合</strong>：每一步都融合了不同层次的特征，从原始输入到高度抽象的表示。</li>
<li><strong>稳定性</strong>：层归一化在每一步后都能稳定激活值，有助于训练的稳定性和收敛速度。</li>
<li><strong>灵活性</strong>：这种结构允许模型在保留必要信息的同时，进行复杂的非线性变换，增强了模型的表达能力。</li>
</ul>
<h2 id="ffn-transformer">前馈神经网络（FFN）在 Transformer 中的作用<a class="headerlink" href="#ffn-transformer" title="Permanent link">&para;</a></h2>
<p>前馈神经网络是 Transformer 架构中的一个关键组件，在编码器和解码器的每一层都有使用。让我们详细探讨它的作用：</p>
<h3 id="ffn">编码器中的 FFN<a class="headerlink" href="#ffn" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>非线性变换</strong></li>
<li>引入非线性：通过激活函数（如 ReLU）引入非线性，增强模型的表达能力。</li>
<li>
<p>复杂特征提取：能够学习和提取更复杂的特征表示。</p>
</li>
<li>
<p><strong>增加模型容量</strong></p>
</li>
<li>参数增加：FFN 通常有较大的隐藏层（如 4 倍的 <span class="arithmatex">\(d_{model}\)</span>），大幅增加模型参数数量。</li>
<li>
<p>学习能力提升：更多参数意味着模型可以学习更复杂的模式和关系。</p>
</li>
<li>
<p><strong>位置特定处理</strong></p>
</li>
<li>独立转换：对每个位置的表示进行独立的转换，补充了自注意力的全局处理。</li>
<li>
<p>局部特征增强：可以捕捉和强化特定位置的特征。</p>
</li>
<li>
<p><strong>信息整合</strong></p>
</li>
<li>综合处理：在自注意力机制之后，进一步处理和整合信息。</li>
<li>特征重组：重新组合和调整通过注意力机制获得的特征。</li>
</ol>
<h3 id="ffn_1">解码器中的 FFN<a class="headerlink" href="#ffn_1" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>功能类似编码器</strong></li>
<li>
<p>在解码器中，FFN 的基本功能与编码器中相同。</p>
</li>
<li>
<p><strong>多源信息处理</strong></p>
</li>
<li>整合多方面信息：在解码器中，FFN 处理的是经过自注意力和交叉注意力后的信息。</li>
<li>
<p>复杂决策：帮助模型在生成输出时做出更复杂的决策。</p>
</li>
<li>
<p><strong>输出准备</strong></p>
</li>
<li>特征调整：为最终的输出层（通常是一个线性层加 softmax）准备合适的特征表示。</li>
</ol>
<h3 id="_7">共同特点<a class="headerlink" href="#_7" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>维度变换</strong></li>
<li>扩展后收缩：通常先将维度从 <span class="arithmatex">\(d_{model}\)</span> 扩展到 <span class="arithmatex">\(d_{ff}\)</span>（如 4 * <span class="arithmatex">\(d_{model}\)</span>），然后再压缩回 <span class="arithmatex">\(d_{model}\)</span>。</li>
<li>
<p>结构：<span class="arithmatex">\(FFN(x) = \max(0, xW_1 + b_1)W_2 + b_2\)</span></p>
</li>
<li>
<p><strong>计算效率</strong></p>
</li>
<li>
<p>并行处理：FFN 可以对序列中的每个位置并行计算，提高效率。</p>
</li>
<li>
<p><strong>残差连接</strong></p>
</li>
<li>与残差连接结合：FFN 的输出通过残差连接与输入相加，有助于信息和梯度的流动。</li>
</ol>
<h3 id="_8">总结<a class="headerlink" href="#_8" title="Permanent link">&para;</a></h3>
<p>前馈神经网络在 Transformer 中扮演着重要角色，它通过增加非线性、提升模型容量、进行位置特定处理来增强模型的表达能力。FFN 与注意力机制相辅相成，共同构建了 Transformer 强大的学习和推理能力。在编码器和解码器中，FFN 帮助模型更好地理解输入序列和生成高质量的输出序列。</p>
<p><a href="https://github.com/InuyashaYang/JoinAI"><img alt="GitHub stars" src="https://img.shields.io/github/stars/InuyashaYang/JoinAI?style=social" /></a></p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/Joining-AI/LLM_Interview_Prepare" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.instant", "navigation.sections", "navigation.tabs"], "search": "../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.56dfad97.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
        <script src="../../javascripts/mathjax_config.js"></script>
      
    
  </body>
</html>